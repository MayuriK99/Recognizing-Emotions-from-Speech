{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import wave\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_audio_data():\n",
    "    x = []\n",
    "    y = []\n",
    "    for file in glob.glob(\"C:\\\\Users\\\\Vaibhav K\\\\Documents\\\\Data Science Project\\\\RES\\\\Dataset\\\\Actor_*\\\\*.wav\"):\n",
    "        file_path=os.path.basename(file)\n",
    "        emotion = emotion_labels[file_path.split(\"-\")[2]]\n",
    "        if emotion not in focused_emotion_labels:\n",
    "            continue\n",
    "        feature = audio_features(file, mfcc=True, chroma=True, mel=True)\n",
    "        \n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    final_dataset = train_test_split(np.array(x), y, test_size=0.1, random_state=9)\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('C:\\\\Users\\\\Vaibhav K\\\\Documents\\\\Data Science Project\\\\RES\\\\Dataset\\\\'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path='.\\Dataset')\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile=os.listdir(dirName)\n",
    "    allFiles=list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath=os.path.join(dirName, entry)\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles=allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "\n",
    "dirName = './Dataset'\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "len(listOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Basic Graphs for understanding of Audio Files :\n",
    "import librosa as lr\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    audio , sfreq = lr.load(listOfFiles[file])\n",
    "    time = np.arange(0 , len(audio)) / sfreq\n",
    "    \n",
    "    fig ,ax = plt.subplots()\n",
    "    ax.plot(time , audio)\n",
    "    ax.set(xlabel = 'Time (s)' , ylabel = 'Sound Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "#PLOT THE SEPCTOGRAM\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "     sample_rate , samples = wavfile.read(listOfFiles[file])\n",
    "     frequencies , times, spectrogram = signal.spectrogram(samples, sample_rate) \n",
    "     plt.pcolormesh(times, frequencies, spectrogram)\n",
    "     plt.imshow(spectrogram)\n",
    "     plt.ylabel('Frequency [Hz]')\n",
    "     plt.xlabel('Time [sec]')\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_features(file_title, mfcc, chroma, mel):\n",
    "    with sf.SoundFile(file_title) as audio_recording:\n",
    "        audio = audio_recording.read(dtype=\"float32\")\n",
    "        sample_rate = audio_recording.samplerate\n",
    "        \n",
    "        if chroma:\n",
    "            stft=np.abs(lb.stft(audio))\n",
    "            result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(lb.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(lb.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(lb.feature.melspectrogram(audio, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = {\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "focused_emotion_labels = ['happy', 'sad', 'angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = loading_audio_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the shape of the training and testing datasets\n",
    "#print((x_train.shape[0], x_test.shape[0]))\n",
    "print((X_train[0], X_test[0]))\n",
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Multi Layer Perception Classifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(200,), learning_rate='adaptive', max_iter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model prediction accuracy score\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy of the Recognizer is: {:.1f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "import pickle\n",
    "# Save the Modle to file in the current working directory\n",
    "#For any new testing data other than the data in dataset\n",
    "\n",
    "Pkl_Filename = \"Emotion_Voice_Detection_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Emotion_Voice_Detection_Model = pickle.load(file)\n",
    "\n",
    "Emotion_Voice_Detection_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting :\n",
    "y_pred=Emotion_Voice_Detection_Model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the Prediction probabilities into CSV file \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "print(y_pred1)\n",
    "y_pred1.to_csv('predictionfinal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
